# Assignment 3

## 学生信息
- 姓名：保护隐私不予显示
- 学号：保护隐私不予显示

## 项目概述

本项目使用物理信息神经网络（PINN）求解三维泊松方程，并实现了多种训练优化机制，包括学习率衰减和自适应采样策略。项目包含基准求解器和PINN求解器两个主要部分。

## 项目结构

```
HW3/
├── Benchmark_Solver/              # 基准求解器
│   ├── 网格法直接求解微分方程/    # 网格法求解器
│   │   ├── poisson_solver.py      # 网格法主程序
│   │   ├── phi.npz               # 网格法求解结果
│   │   └── README.md             # 网格法说明文档
│   └── DMC法求解微分方程/        # DMC法求解器
│       └── diffusion_monte_carlo_poisson.py  # DMC法主程序
├── code/                          # PINN求解器代码
│   ├── train_无学习率衰减、无loss自适应.py    # 基础版本
│   ├── train_学习率衰减机制.py             # 学习率衰减版本
│   ├── train_自适应采样机制.py             # 自适应采样版本
│   ├── exam_absulote.py          # 模型评估程序
│   ├── check_cuda.py             # CUDA检测程序
│   └── *.pth                     # 训练好的模型文件
└── result/                        # 训练结果
    ├── 无学习率衰减、无loss自适应，训练曲线.png
    ├── 有学习率衰减、无loss自适应，训练曲线.png
    └── 有学习率衰减、有loss自适应，训练曲线.png
```

## 问题描述

在立方体域 [-1,1]^3 上，求解泊松方程：
```
∇²φ = -ρ
```
边界条件：φ = 0（Dirichlet边界条件）
电荷分布：ρ = 100 * x * y * z²（使用自然单位制，ε₀ = 1）

## 使用指南

### 1. 基准求解器使用

#### 网格法求解器
```bash
cd Benchmark_Solver/网格法直接求解微分方程
python poisson_solver.py --px 128 --out phi_128.npz
```

**使用特点**：
- 使用离散正弦变换（DST-I）将离散拉普拉斯算子对角化
- 复杂度约为 O(n³ log n)，计算效率高
- 结果可视化：可以拖动滑块观察不同截面的电势分布

#### DMC法求解器
```bash
cd Benchmark_Solver/DMC法求解微分方程
python diffusion_monte_carlo_poisson.py
```
**注意事项**：
- 扩散蒙特卡洛方法计算成本较高
- 可能需要较长的运行时间才能获得精确结果
- 建议用于验证其他方法的正确性

### 2. PINN求解器运行

#### 基础版本（无优化机制）
```bash
cd code
python train_无学习率衰减、无loss自适应.py
```

#### 学习率衰减版本
```bash
cd code
python train_学习率衰减机制.py
```

#### 自适应采样版本（推荐）
```bash
cd code
python train_自适应采样机制.py
```

### 3. 模型评估与结果检验
```bash
cd code
python exam_absulote.py
```
**评估方法**：
- 使用训练好的PINN模型进行预测
- 与基准求解器结果进行对比
- 可视化不同截面的电势分布

**结果特点**：
- **中间区域符合较好**：模型在立方体内部区域能够较好地拟合基准解
- **边界处理仍有问题**：边界条件的拟合效果仍有改进空间
- **可视化分析**：通过拖动滑块可以观察不同z截面的电势分布，直观比较PINN预测与基准解的差异

**使用方式**：
1. 运行程序后会显示交互式图形界面
2. 使用滑块选择不同的z截面
3. 观察PINN预测结果与基准解的对比
4. 分析边界区域的拟合效果


## 主要功能

### 1. 物理信息神经网络（PINN）
- **网络结构**: 5层隐藏层，每层256个神经元，使用tanh激活函数
- **损失函数**: PDE损失 + 边界损失
- **优化器**: Adam优化器，初始学习率1e-3

### 2. 学习率衰减机制
- **触发条件**: 400个epoch后开始学习率衰减
- **衰减策略**: 基于损失改进率的自适应衰减
- **观察窗口**: 100个epoch的滑动窗口实时监测是否应该衰减学习率

### 3. 自适应采样机制
- **采样比例自适应**: 根据边界损失改进贡献度动态调整边界采样比例
- **损失权重调控**: 使用beta参数调控边界损失的相对重要性
- **归一化保证**: 总损失按采样概率进行归一化

### 4. 训练监控
- **实时绘图**: 显示总损失、PDE损失、边界损失曲线
- **学习率监控**: 显示学习率变化曲线
- **采样比例监控**: 显示边界采样比例变化
- **手动终止**: 提供图形界面手动终止训练按钮

## 主要结果

### 1. 训练曲线对比
- **基础版本**: 训练过程相对缓慢，收敛效果一般
- **学习率衰减版本**: 训练后期收敛速度提升
- **自适应采样版本**: 训练效率最高，收敛效果最好

### 2. 新版本模型更新说明
**更新时间**: 2024年11月12日  
**新模型位置**: `/home/stu2200011534/assignments/assignment3/result/pinn_new.pth`

**具体更改内容**:
- **超参数优化**: 更新了自适应采样机制的关键参数
  - `beta = 10` (之前版本为了演示自适应采样机制，故意设置较低值)
  - `base_ratio = 0.1` (基础边界采样比例)
- **训练效果**: 
  - 训练轮次: 1783 epochs
  - 训练时间: 566.16秒
  - 达成较好的收敛效果
- **机制说明**: 没有修改任何核心机制，仅通过优化超参数获得更好的训练效果

**技术背景**:
- 之前的版本为了演示自适应采样机制的工作效果，故意将beta参数调低
- 这样在初始采样阶段（自适应机制不工作时），边界误差会很大
- 自适应机制能够发现这个误差并自动提高边界采样比例
- 新版本通过合理的超参数设置，从一开始就获得了更好的训练效果

### 3. 学习率衰减机制
- **智能衰减**: 基于损失改进率的自适应衰减策略
- **稳定训练**: 避免训练后期学习率过大导致的震荡
- **效率提升**: 在训练后期自动降低学习率，提高收敛精度

### 4. 自适应采样机制优势
- **动态权重分配**: 根据边界损失改进贡献度自动调整采样比例
- **归一化损失**: 使用公式 `(P(采样点在体)*loss_pde + beta*P(采样点在边)*loss_boundary) / (P(采样点在体) + beta*P(采样点在边))`
- **参数可调**: beta参数可灵活调整边界条件的重要性

## 技术迭代过程

### 1. 基础版本的问题识别
在完成基础版本后，发现两个主要问题：
- **边界处理不佳**: 边界条件拟合效果不理想
- **ADAM优化器震荡**: 训练过程中loss经常突然增大后再衰减回来，影响训练稳定性

### 2. 学习率衰减机制的引入
为了解决loss震荡问题，引入了学习率下降机制：
- **ADAM优化器的补充**: ADAM利用动量机制防止一阶导太小的地方下降率太小
- **学习率衰减的作用**: 防止一阶导太小但二阶导较大时下降率过大
- **效果验证**: 从loss曲线可以看出对震荡的明显改进

### 3. 自适应采样机制的开发
针对边界解不准的问题，开发了自适应采样机制：
- **边界采样增强**: 动态增加边界采样点及其在loss函数中的影响占比
- **机制协同**: 结合学习率下降机制可以有效减少超参数选择不当导致的震荡
- **边界处理改进**: 相比原始版本，边界条件的拟合效果得到显著提升

### 4. 迭代过程总结与局限
- **机制生效时机**: 两种优化机制均在训练400轮后开始生效
- **前期错误修正**: 难以完全纠正模型在训练前期已经形成的边界不准问题
- **优化方向**: 需要更好的超参数调节策略，而非依赖后期修正机制来矫正前期错误

## 系统优势与创新点

### 1. 多机制协同优化
- **自适应采样**: 根据训练状态动态调整采样策略
- **学习率衰减**: 智能控制训练步长
- **损失归一化**: 保证数值稳定性

### 2. 可视化监控
- **实时训练曲线**: 多维度训练状态监控
- **交互式控制**: 手动终止训练功能
- **完整日志**: 详细的训练过程记录

### 3. 高度可配置
- **参数灵活调整**: 所有关键参数均可配置
- **模块化设计**: 各优化机制独立可开关
- **易于扩展**: 支持新优化机制的快速集成

## 基于Cline的编程辅助过程

### 1. 代码优化与重构
- 在Cline辅助下实现了自适应采样机制的优化
- 重构了损失函数计算逻辑，提高了代码可读性
- 整理程序可读性和参数统一管理

### 2. 功能增强
- **beta参数引入**: 在Cline指导下添加了边界权重调控参数
- **损失归一化**: 实现了基于采样概率的损失归一化机制
- **参数调整**: 优化了各项超参数的默认值

### 3. 文档完善
- 在Cline协助下编写了完整的项目文档
- 统一了代码注释和文档格式
- 完善了运行说明和参数解释

## 注意事项

1. **训练时间**: 完整训练在2500轮900秒左右
2. **内存使用**: 训练过程中会占用较多内存，建议关闭其他大型程序；我的cuda因为显卡不兼容原因无法使用，或许可以改进
3. **图形界面**: 训练过程中会弹出图形窗口，请勿关闭
4. **手动终止**: 如需提前终止训练，可点击图形界面上的"停止训练"按钮

## 未来改进方向

1. **更精细的采样策略**: 进一步优化自适应采样算法，使得即使预训练成果不好也能通过优化的采样修正
2. **多目标优化**: 考虑更多训练目标的平衡
3. **并行计算**: 实现多GPU并行训练
4. **模型震荡行为的探究**：找到震荡原因并想办法抑制
